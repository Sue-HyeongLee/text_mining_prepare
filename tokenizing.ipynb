{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hkqQU-2J7RS",
        "outputId": "0e73fd6d-5c5b-49d9-a681-9b6e42607f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('webtext')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 단위 토큰화.\n",
        "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize # 문장 단위로 토큰화 ! . ? 등을 이용\n",
        "\n",
        "print(sent_tokenize(para))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVf8J9GEKMQm",
        "outputId": "65be4446-c639-427b-9ca1-5645a22df31f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 만약 프랑스어 같은 걸로 하고 싶다면 punkt 사용.\n",
        "\n",
        "\"\"\"\n",
        "import nltk.data\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
        "print(tokenizer.tokenize(paragraph_french))\n",
        "\"\"\"\n",
        "# 한국어도 잘 작동할 것임. . ! ? 등을 이용했기에.\n",
        "\n",
        "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트 마이닝 클래스를 시작해봅시다!\"\n",
        "\n",
        "print(sent_tokenize(para_kor))\n",
        "# 별 다른 문제 없이 작동함을 볼 수 있음."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cqb3YiAK3wd",
        "outputId": "3238f444-7e1e-49b8-9411-5c562d00bebc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트 마이닝 클래스를 시작해봅시다!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화.\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 주어진 텍스트를 word 단위로 토크나이즈 함.\n",
        "print(word_tokenize(para))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l7vIorrLz2G",
        "outputId": "85f03dfb-0585-456a-8f8e-d4df3e8183a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "print(WordPunctTokenizer().tokenize(para)) # 토크나이저가 서로 다른 알고리즘을 사용하기에 이는, 자신의 필요성에 맞게 선택할 필요가 있음."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVtlTOoIMZxV",
        "outputId": "747a3d1e-bc25-4f85-bee6-cf2f4aac18a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(para_kor)) # 한국어에서는 의미 단위로 처리되지 않음. 이는 영어는 공백 단위로 분리되지만, 한국어는 아니기 때문. 새로운 단어 분할이 필요함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp6CpVlRMjwh",
        "outputId": "3ebae6fd-db2c-49f5-9cc5-5eeef686b3b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트', '마이닝', '클래스를', '시작해봅시다', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규 표현식을 이용한 토큰화.\n",
        "import re\n",
        "re.findall(\"[abc]\", \"How are you, boy?\") # 문자를 찾고 싶다면."
      ],
      "metadata": {
        "id": "2eyr9oioM3gy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7547a7-f2ce-467b-bf8c-9e1efd6d8ba5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'b']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[0123456789]\", \"3a7b5c9d\") # 숫자를 찾고 싶다면."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSLF00EEkq2Z",
        "outputId": "dc14d2e5-d114-4c22-a27f-a29d76476f78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3', '7', '5', '9']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[\\w]\", \"3a 7b_ '.^&5c9d\") # [a-zA-z0-9_]는 [\\w]로 표현 가능."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_d25Rh7k28I",
        "outputId": "b95212d8-77ec-476c-de59-ce938255cd90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[a_]+\", \"aa_b, ac__d, e__f\") # +는 한번 이상 반복한 거"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYwuzQIflW9-",
        "outputId": "11bea853-ba3d-41a6-fd98-4becac6e2a9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aa_', 'a', '__', '__']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[\\w]+\", \"How are you, boy?\") # 공백이나 쉼표가 포함되지 않음. 공백이나 쉼표 등으로 구분되는 단어들을 찾아낼 수 있음."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfjgTvyMlq4W",
        "outputId": "bd1002ca-2098-40e8-810d-d9f2ac80996c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How', 'are', 'you', 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[a_]{2,4}\", \"aa_b, a_c__d, e__f\") # {2,4}는 +와 달리, 반복 횟수를 지정할 수 있음."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5uzSgGCmQvy",
        "outputId": "4123f9fe-fdc4-4ff1-9943-ef45e68c08cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aa_', 'a_', '__', '__']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[o]{2,4}\", \"oh, hoow are yoooou, boooooooy?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPvAOuzamZUr",
        "outputId": "c6bb8af2-21bc-4f63-a6dd-c9c4ef15ffa0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oo', 'oooo', 'oooo', 'ooo']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# regular expression(정규식)을 이용한 tokenizer\n",
        "# 단어 단위로 tokenize \\w:문자나 숫자를 의미. 즉 문자나 숫자 혹은 '가 반복되는 것을 찾아냄.\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "print(tokenizer.tokenize(\"Sorry, I can't go there.\")) # 문자, 숫자, _외 '까지 포함. 그 외는 단어를 구분하는 단어 경계로 사용."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he6VOtRqm1-O",
        "outputId": "3d1110c9-0717-4afd-a324-11efba689c37"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sorry', 'I', \"can't\", 'go', 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\w]+\") # '를 뺀다면?\n",
        "print(tokenizer.tokenize(\"Sorry, I can't go there.\")) # can't는 can과 t로 나뉘어진다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hzp3YcinyP_",
        "outputId": "a5249d36-c223-411a-904f-1985f633b08f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sorry', 'I', 'can', 't', 'go', 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소문자로 바꾸고, '를 포함해 세 글자 이상의 단어들.\n",
        "text1 = \"Sorry, I can't go there.\"\n",
        "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")\n",
        "print(tokenizer.tokenize(text1.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1x9JAoloCR2",
        "outputId": "c5ae4605-0321-4f44-8f63-5ca85c313db9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', \"can't\", 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 사전을 이용해 불용어 제거. 불용어란 실제로 사용하지만, 쓸모 없는 단어,\n",
        "from nltk.corpus import stopwords\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "tokens = tokenizer.tokenize(text1.lower())\n",
        "\n",
        "result = [word for word in tokens if word not in english_stops]\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C3_AlhCom8E",
        "outputId": "445dc035-a31a-47b8-f727-bb11c8d8fcae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', 'go', 'movie', 'yesterday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_stops) # 불용어에 포함된 것."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAuXE6I8qQCA",
        "outputId": "f2845bdd-f389-4c81-e343-05a02825693a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"that'll\", 'only', 'o', 'yourself', 'from', 'while', 'hasn', 'more', 'after', 'those', 'her', 'now', 'it', \"you've\", \"weren't\", 'most', 'but', 'himself', 'my', \"shouldn't\", 'own', 'if', 'into', 'll', 'out', 'over', 'a', 'of', 'did', 'that', 'their', 'other', 'same', 'why', 't', 'again', 'd', \"doesn't\", 'under', 'y', 'below', 'didn', 'she', \"wasn't\", 'our', 'ourselves', \"needn't\", 'before', 'off', 'yourselves', 'between', 'up', \"don't\", 're', 'won', \"you'd\", 'these', 'has', 'couldn', 'needn', 'weren', 'so', 'wasn', 'about', 'shan', \"mightn't\", 'haven', 'nor', 'ours', 'doing', 'we', 'once', \"won't\", \"you'll\", 'mightn', 'be', 'on', 'whom', 'further', 'when', 'yours', \"mustn't\", 'itself', \"hasn't\", 'no', 'just', 'few', \"haven't\", 'how', \"it's\", 'until', 'above', 'by', 'theirs', 'very', 'who', 'some', 'because', 'can', 'being', \"isn't\", 'were', 'during', 'such', 'too', 'not', 'have', 'doesn', 'and', \"shan't\", 'at', 'as', 'will', 'or', 'through', 'any', \"should've\", 'are', \"she's\", 'which', 'was', 'the', \"hadn't\", 'here', 'against', 'ma', 'am', 'aren', 'wouldn', 'ain', 'his', 'been', 'to', 'its', 'in', 'is', 'don', 'your', 'where', 'm', \"wouldn't\", 'you', 'he', 'mustn', 'they', 'do', 'him', \"you're\", 'herself', 'them', 'than', 'this', 'with', 'should', 'there', \"didn't\", 'each', 'had', 've', 'shouldn', 'i', 'all', 'hadn', 'an', 'themselves', 'then', 'isn', 's', 'myself', 'both', 'me', 'down', 'what', \"aren't\", 'does', 'having', 'for', 'hers', \"couldn't\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자신만의 불용어 사전을 만들기도 가능.\n",
        "my_stopword = ['i', 'go', 'to']\n",
        "result = [word for word in tokens if word not in my_stopword]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12N2sHijqhkC",
        "outputId": "e6a72c2f-e3c2-4c27-ec7d-b2d729695774"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CayirbXNq-D5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}